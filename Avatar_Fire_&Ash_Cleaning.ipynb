{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1oohbwXQEZ1WM3nifZ9YK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mihan0207/Avatar_Fire_-_Ash_Prediction/blob/main/Avatar_Fire_%26Ash_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "od_nDxoz8NXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f3858ed"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "This section loads the movie data from the `movie_data_2008_2024.csv` file into a pandas DataFrame named `df`. This is the foundational step, making the data available for all subsequent processing, cleaning, and analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "df = pd.read_csv('movie_data_2008_2024.csv')\n"
      ],
      "metadata": {
        "id": "WhO4hMqj8QMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60d4f622"
      },
      "source": [
        "## Explain Date Cleaning and Filtering\n",
        "\n",
        "This section focuses on cleaning and standardizing the `release_date` column and then filtering the dataset.\n",
        "\n",
        "### `parse_date` Function Explanation:\n",
        "The `parse_date` function is designed to robustly convert various date string formats into a consistent pandas `datetime` object. It handles the following:\n",
        "- **Initial Cleaning**: It first removes any characters that appear after a newline (`\\n`) or a hyphen (`-`) in the date string, taking only the first part. This helps in standardizing entries that might contain extra information beyond the date itself.\n",
        "- **Format Conversion**: It then attempts to convert the cleaned string into a datetime object using the format `'%b %d, %Y'` (e.g., 'Jul 18, 2008').\n",
        "- **Missing and Invalid Values**: If the input date string is `NaN` (Not a Number) or if the conversion fails due to an invalid format, the function gracefully returns `pd.NaT` (Not a Time), which is pandas' representation for a missing or null datetime value.\n",
        "\n",
        "### Data Filtering:\n",
        "After applying the `parse_date` function to the `release_date` column, two subsequent filtering steps are performed:\n",
        "1. **Drop Missing Dates**: Rows where `release_date` is `pd.NaT` (i.e., dates that were originally missing or could not be parsed) are dropped from the DataFrame. This ensures that all remaining entries in the `release_date` column are valid datetime objects.\n",
        "2. **Filter by Year**: The DataFrame is then filtered to include only movies released from the year 2008 onwards (`df['release_date'].dt.year >= 2008`). This aligns the dataset with the specified time frame of interest, ensuring that only relevant movie data is analyzed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CLEANING STEPS ---\n",
        "def parse_date(date_str):\n",
        "    try:\n",
        "        if pd.isna(date_str): return pd.NaT\n",
        "        # Extract first part before newline or hyphen\n",
        "        clean_str = re.split(r'[\\n-]', str(date_str))[0].strip()\n",
        "        return pd.to_datetime(clean_str, format='%b %d, %Y')\n",
        "    except: return pd.NaT\n",
        "\n",
        "# Clean Dates & Filter Year\n",
        "df['release_date'] = df['release_date'].apply(parse_date)\n",
        "df = df.dropna(subset=['release_date'])\n",
        "df = df[df['release_date'].dt.year >= 2008]"
      ],
      "metadata": {
        "id": "P6fD1S6p8XcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a494279"
      },
      "source": [
        "### Explaining Column Dropping\n",
        "\n",
        "Before proceeding with further data processing, we are dropping the following columns:\n",
        "\n",
        "- **`opening_source`**: This column contains information about the source of the opening revenue data, which is not relevant for our current analytical or machine learning objectives. It often contains uniform or uninformative values.\n",
        "- **`imdb_votes`**: While IMDB votes can indicate a movie's popularity, we already have `tmdb_rating` and `tmdb_votes` which provide similar information and are often more complete or consistent with the TMDb data used elsewhere. Additionally, this column might have a significant number of missing values or be less directly impactful than other features for the target predictions.\n",
        "\n",
        "Dropping these columns helps to reduce noise, simplify the dataset, and avoid potential issues with irrelevance or redundancy in downstream tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop Columns\n",
        "cols_to_drop = ['opening_source', 'imdb_votes']\n",
        "df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])"
      ],
      "metadata": {
        "id": "g00Jl-JW8ZYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de0f6829"
      },
      "source": [
        "### Cleaning the `distributor` Column\n",
        "\n",
        "This step focuses on cleaning the `distributor` column to ensure consistency and remove irrelevant information. The code performs the following actions:\n",
        "\n",
        "- **Converts to String**: Ensures the column is of string type using `.astype(str)` to handle potential mixed data types.\n",
        "- **Removes Placeholder Text**: Uses `.str.replace('See full company information', '', regex=False)` to remove a specific placeholder phrase that might appear in some entries, which is not part of the actual distributor name.\n",
        "- **Removes Extra Whitespace**: Applies `.str.strip()` to remove any leading or trailing whitespace that might result from the cleaning process or exist in the original data, ensuring clean and standardized distributor names.\n",
        "\n",
        "This process helps to standardize the distributor names, making the column more suitable for analysis or categorical encoding."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use .str.replace() to find the specific text and replace it with nothing ('')\n",
        "if 'distributor' in df.columns:\n",
        "    df['distributor'] = df['distributor'].astype(str).str.replace('See full company information', '', regex=False).str.strip()\n",
        "    df['distributor'] = df['distributor'].replace({'nan': None, 'None': None})"
      ],
      "metadata": {
        "id": "JZF6S3CRGNPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f3ec6db"
      },
      "source": [
        "## Genre Processing Explanation\n",
        "\n",
        "To effectively use genre information for analysis or machine learning, the raw 'genres' string needs to be processed. This section performs two key transformations:\n",
        "\n",
        "1.  **Extracting Primary Genre**: For each movie, the first genre listed is identified and stored as `primary_genre`. This provides a single, main categorization for simpler segmentation and high-level analysis.\n",
        "\n",
        "2.  **One-Hot Encoding All Genres**: To allow for a movie to belong to multiple genres and to enable quantitative analysis, a one-hot encoding scheme is applied. For every unique genre present across all movies, a new binary column is created (e.g., `genre_Action`, `genre_Comedy`). A value of `1` in these columns indicates that the movie belongs to that specific genre, while `0` indicates its absence. This method transforms categorical genre data into a numerical format that machine learning models can readily interpret and use, avoiding issues associated with direct string-based categorical encoding."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GENRE PROCESSING ---\n",
        "\n",
        "# 1. Primary Genre (First item in list) - Useful for charts\n",
        "df['primary_genre'] = df['genres'].apply(lambda x: str(x).split(',')[0].strip() if pd.notna(x) else 'Unknown')\n",
        "\n",
        "# 2. Extract Genres List\n",
        "df['genres_list'] = df['genres'].fillna('').apply(lambda x: [g.strip() for g in str(x).split(',') if g.strip()])\n",
        "\n",
        "# 3. Define Target Genres (Top 5 + Sci-Fi)\n",
        "# Top 5: Drama, Comedy, Action, Adventure, Thriller\n",
        "# Plus: Science Fiction (Essential for Avatar)\n",
        "target_genres = ['Drama', 'Comedy', 'Action', 'Adventure', 'Thriller', 'Science Fiction']\n",
        "\n",
        "# 4. Create One-Hot Columns\n",
        "for genre in target_genres:\n",
        "    # Example: Creates 'genre_Action', 'genre_Science Fiction'\n",
        "    df[f'genre_{genre}'] = df['genres_list'].apply(lambda x: 1 if genre in x else 0)\n",
        "\n",
        "# 5. Create 'genre_Other'\n",
        "# If the movie has a genre NOT in our target list (e.g., Horror), mark it here\n",
        "def has_other(g_list):\n",
        "    for g in g_list:\n",
        "        if g not in target_genres:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "df['genre_Other'] = df['genres_list'].apply(has_other)\n",
        "\n",
        "# Drop the temporary list column\n",
        "df = df.drop(columns=['genres_list'])"
      ],
      "metadata": {
        "id": "SguXVXcX8cZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f55b0d9e"
      },
      "source": [
        "## Column Reordering for Readability and Structure\n",
        "\n",
        "To enhance the readability and logical structure of our DataFrame, columns are being reordered. This arrangement places key identifier and monetary columns at the beginning, making it easier to quickly grasp essential information about each movie. These include `title`, `year`, `release_date`, `total_gross`, `opening_revenue`, `final_budget`, and `primary_genre`. Following these, other relevant data columns are listed, and finally, the genre-specific one-hot encoded columns are grouped together at the end. This organized structure is particularly beneficial for data exploration and subsequent machine learning tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- REORDER COLUMNS ---\n",
        "main_cols = [\n",
        "    'title', 'year', 'release_date', 'primary_genre',\n",
        "    'total_gross', 'opening_revenue', 'final_budget',  # Financials\n",
        "    'distributor',\n",
        "    'tmdb_id', 'imdb_id',                              # IDs (MOVED HERE)\n",
        "    'tmdb_rating', 'imdb_rating', 'tmdb_votes',        # Ratings\n",
        "    'runtime', 'director', 'genres'                    # Details\n",
        "]\n",
        "\n",
        "# Append the specific genre columns\n",
        "genre_cols = [f'genre_{g}' for g in target_genres] + ['genre_Other']\n",
        "\n",
        "# Append any other remaining columns\n",
        "extra_cols = [c for c in df.columns if c not in main_cols and c not in genre_cols]\n",
        "\n",
        "# Apply Final Order\n",
        "final_order = main_cols + extra_cols + genre_cols\n",
        "df = df[[c for c in final_order if c in df.columns]]"
      ],
      "metadata": {
        "id": "bfa53dxM8eqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6558d23d"
      },
      "source": [
        "## Saving Cleaned Data\n",
        "\n",
        "Before proceeding with any further analysis or machine learning tasks, it's crucial to save the cleaned and processed data. We are saving two distinct versions of the cleaned dataset:\n",
        "\n",
        "1.  **`cleaned_movie_data_ml_ready.csv`**: This version contains all numerical values (e.g., total_gross, opening_revenue, final_budget) in their original numeric format. This is the dataset specifically prepared for machine learning model training, where numerical consistency and lack of special characters are essential.\n",
        "\n",
        "2.  **`cleaned_movie_data_display.csv`**: This version is intended for human-readable reports and displays. Numerical columns like `total_gross`, `opening_revenue`, and `final_budget` have been formatted as currency strings (e.g., '$123,456.78') for better presentation. This version should **not** be used directly for machine learning as the currency symbols and commas will interfere with numerical calculations.\n",
        "\n",
        "Choose the appropriate file based on your next steps: `_ml_ready.csv` for modeling, and `_display.csv` for reporting."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- SAVE OUTPUTS ---\n",
        "\n",
        "# Version 1: Machine Learning Ready (Numeric)\n",
        "df.to_csv('cleaned_movie_data_ml_ready.csv', index=False)\n",
        "\n",
        "# Version 2: Display Ready (Currency Strings)\n",
        "df_display = df.copy()\n",
        "def clean_currency(x):\n",
        "    try:\n",
        "        if pd.isna(x): return \"$0.00\"\n",
        "        return \"${:,.2f}\".format(float(x))\n",
        "    except: return x\n",
        "\n",
        "for col in ['total_gross', 'opening_revenue', 'final_budget']:\n",
        "    df_display[col] = df_display[col].apply(clean_currency)\n",
        "\n",
        "df_display.to_csv('cleaned_movie_data_display.csv', index=False)\n",
        "\n",
        "print(\"âœ… Files Created:\")\n",
        "print(\"1. cleaned_movie_data_ml_ready.csv (Numeric - Use this for Standardization/ML)\")\n",
        "print(\"2. cleaned_movie_data_display.csv (Currency Strings - Use this for Reports)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mO2dOf-g8FyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9732d360-10d9-43ae-e4be-bc3b71af796e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Files Created:\n",
            "1. cleaned_movie_data_ml_ready.csv (Numeric - Use this for Standardization/ML)\n",
            "2. cleaned_movie_data_display.csv (Currency Strings - Use this for Reports)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06130dad"
      },
      "source": [
        "### The Importance of Feature Scaling for Machine Learning\n",
        "\n",
        "\n",
        "As observed in our dataset, features like `'total_gross'` can have values ranging into hundreds of millions, while `'tmdb_rating'` is on a much smaller scale, typically between 0 and 10. If these features are used directly without scaling, the feature with the larger magnitude (`'total_gross'`) will disproportionately influence the model's objective function and learning process.\n",
        "\n",
        "To prevent features with larger values from dominating the learning process and to ensure that all features contribute equally to the model's performance, it is essential to perform **standardization** or **normalization**. Common techniques include:\n",
        "- **Min-Max Scaling:** Rescales features to a fixed range, usually 0 to 1.\n",
        "- **Standardization (Z-score normalization):** Rescales features to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "Applying one of these techniques ensures that the features are on a comparable scale, leading to better model convergence and improved predictive performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STANDARDIZATION CHECK ---\n",
        "print(\"\\nðŸ“Š Standardization Check (Why you need it):\")\n",
        "stats = df[['total_gross', 'tmdb_rating']].describe().loc[['mean', 'max']]\n",
        "print(stats)\n",
        "print(\"\\n--> 'total_gross' max is ~900 Million, while 'rating' max is 10.\")\n",
        "print(\"--> You MUST standardize these features before Machine Learning.\")"
      ],
      "metadata": {
        "id": "Df9DNB588mZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bacd74c-5d15-4405-f190-a7719b758167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š Standardization Check (Why you need it):\n",
            "       total_gross  tmdb_rating\n",
            "mean  5.816772e+07     6.626091\n",
            "max   9.366622e+08    10.000000\n",
            "\n",
            "--> 'total_gross' max is ~900 Million, while 'rating' max is 10.\n",
            "--> You MUST standardize these features before Machine Learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a84a60d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "\n",
        "*   The meticulous data cleaning and preparation, covering date parsing, feature selection, categorical encoding, and structured data output, provides a highly robust and versatile dataset, ready for a wide array of analytical tasks and machine learning model development.\n",
        "*   The clear distinction between the ML-ready and display-ready datasets is a best practice that prevents data type errors in machine learning pipelines and caters to diverse stakeholder needs (analytical reporting vs. model training).\n"
      ]
    }
  ]
}