{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 14165635,
          "sourceType": "datasetVersion",
          "datasetId": 9029347
        }
      ],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mihan0207/Avatar_Fire_and_Ash_Prediction/blob/main/Avatar_Fire%26Ash_Revenue_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction and Library Imports\n",
        "\n",
        "This initial cell serves two primary purposes:\n",
        "\n",
        "1.  **Project Overview:** It provides a high-level description of the machine learning project,\n",
        "    which aims to predict the opening revenue for movies, with a specific focus on\n",
        "    \"Avatar: Fire and Ash\". It also lists the various ML models that will be compared\n",
        "    for this prediction task.\n",
        "\n",
        "2.  **Environment Setup:** It imports all necessary Python libraries required for\n",
        "    data manipulation (pandas, numpy, datetime), machine learning (sklearn, xgboost,\n",
        "    lightgbm, catboost), and data visualization (matplotlib, seaborn).\n",
        "    Warnings are also configured to be ignored for cleaner output.\n",
        "\n",
        "This cell prepares the environment and sets the stage for the subsequent data loading,\n",
        "preprocessing, modeling, and evaluation steps."
      ],
      "metadata": {
        "id": "K6jdGM8AnZvm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32fe775a",
        "outputId": "6198290e-720d-4c85-a48c-b6206a1cdaf7"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML Libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T14:45:40.680977Z",
          "iopub.execute_input": "2025-12-15T14:45:40.681409Z",
          "iopub.status.idle": "2025-12-15T14:45:40.688539Z",
          "shell.execute_reply.started": "2025-12-15T14:45:40.681383Z",
          "shell.execute_reply": "2025-12-15T14:45:40.687330Z"
        },
        "id": "WZ4T67_ekHWL"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eda3de6"
      },
      "source": [
        "### Step 1: Load and Explore Data\n",
        "\n",
        "This cell is responsible for loading the initial datasets required for the movie revenue prediction project. It loads three CSV files into pandas DataFrames: `cleaned_movie_data_ml.csv` (main movie data), `dim_director.csv` (director-specific information), and `dim_distributor.csv` (distributor-specific information). After loading, it prints the dimensions (rows and columns) of each DataFrame and lists their respective column names, providing an immediate overview of the available data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 1: LOAD AND EXPLORE DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 1: LOADING DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load all three datasets\n",
        "movies_df = pd.read_csv('/content/cleaned_movie_data_ml.csv')\n",
        "directors_df = pd.read_csv('/content/dim_director.csv')\n",
        "distributors_df = pd.read_csv('/content/dim_distributor.csv')\n",
        "\n",
        "print(f\"\\nüìä Movies Dataset: {movies_df.shape[0]} rows, {movies_df.shape[1]} columns\")\n",
        "print(f\"üìä Directors Dataset: {directors_df.shape[0]} rows, {directors_df.shape[1]} columns\")\n",
        "print(f\"üìä Distributors Dataset: {distributors_df.shape[0]} rows, {distributors_df.shape[1]} columns\")\n",
        "\n",
        "# Display column names\n",
        "print(\"\\nüîπ Movies Columns:\", list(movies_df.columns))\n",
        "print(\"\\nüîπ Directors Columns:\", list(directors_df.columns))\n",
        "print(\"\\nüîπ Distributors Columns:\", list(distributors_df.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-BECxrLp6_v",
        "outputId": "27dbdde1-e7eb-4bb4-e9e3-d451db1adfa3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 1: LOADING DATA\n",
            "================================================================================\n",
            "\n",
            "üìä Movies Dataset: 3332 rows, 23 columns\n",
            "üìä Directors Dataset: 1656 rows, 7 columns\n",
            "üìä Distributors Dataset: 186 rows, 7 columns\n",
            "\n",
            "üîπ Movies Columns: ['title', 'year', 'release_date', 'primary_genre', 'total_gross', 'opening_revenue', 'final_budget', 'distributor', 'tmdb_id', 'imdb_id', 'tmdb_rating', 'imdb_rating', 'tmdb_votes', 'runtime', 'director', 'genres', 'genre_Drama', 'genre_Comedy', 'genre_Action', 'genre_Adventure', 'genre_Thriller', 'genre_Science Fiction', 'genre_Other']\n",
            "\n",
            "üîπ Directors Columns: ['director_name', 'total_movies', 'avg_total_gross', 'avg_opening_revenue', 'avg_final_budget', 'avg_tmdb_rating', 'director_id']\n",
            "\n",
            "üîπ Distributors Columns: ['distributor_name', 'total_movies', 'total_gross', 'avg_opening_revenue', 'avg_final_budget', 'market_share', 'distributor_id']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cdd9ee4"
      },
      "source": [
        "### Step 2: Data Preprocessing and Feature Engineering\n",
        "\n",
        "This cell performs critical steps to prepare the raw data for machine learning models. It involves:\n",
        "\n",
        "1.  **Data Copying:** Creates a working copy of the `movies_df` to ensure the original DataFrame remains untouched.\n",
        "2.  **Date Parsing & Temporal Feature Extraction:** Converts the `release_date` column into a datetime object and extracts various temporal features such as `release_month`, `release_day`, `release_dayofweek`, `release_quarter`, and `release_weekofyear`. It also generates binary indicators for `is_holiday_release`, `is_summer_release`, and `is_weekend_release`.\n",
        "3.  **Merging Director Information:** Integrates director-specific aggregated data from `directors_df` into the main DataFrame, adding features like `director_total_movies`, `director_avg_opening_revenue`, `director_avg_budget`, etc.\n",
        "4.  **Merging Distributor Information:** Incorporates distributor-specific aggregated data from `distributors_df`, bringing in features such as `distributor_total_movies`, `distributor_avg_opening_revenue`, and `distributor_market_share`.\n",
        "5.  **Creation of Additional Features:** Derives new features to enhance predictive power:\n",
        "    *   `budget_per_rating`: Ratio of `final_budget` to `tmdb_rating`.\n",
        "    *   `genre_count`: Number of genres associated with a movie.\n",
        "    *   `is_major_studio`: A binary flag indicating if the distributor is one of the predefined major studios.\n",
        "    *   `director_experience`: Categorizes directors based on their total number of movies.\n",
        "6.  **Missing Value Handling:** Identifies and imputes missing numerical values using the median. Categorical IDs (`director_id`, `distributor_id`) are filled with -1, and `director_experience` with 1, to ensure data completeness for modeling."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 2: DATA PREPROCESSING AND FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: DATA PREPROCESSING & FEATURE ENGINEERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 2.1 Create a copy for processing\n",
        "df = movies_df.copy()\n",
        "\n",
        "# 2.2 Parse release date and extract temporal features\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Parse various date formats\"\"\"\n",
        "    try:\n",
        "        # Try MM/DD/YY format\n",
        "        return pd.to_datetime(date_str, format='%m/%d/%y')\n",
        "    except:\n",
        "        try:\n",
        "            # Try other formats\n",
        "            return pd.to_datetime(date_str)\n",
        "        except:\n",
        "            return pd.NaT\n",
        "\n",
        "df['release_date_parsed'] = df['release_date'].apply(parse_date)\n",
        "\n",
        "# Extract temporal features\n",
        "df['release_month'] = df['release_date_parsed'].dt.month\n",
        "df['release_day'] = df['release_date_parsed'].dt.day\n",
        "df['release_dayofweek'] = df['release_date_parsed'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
        "df['release_quarter'] = df['release_date_parsed'].dt.quarter\n",
        "df['release_weekofyear'] = df['release_date_parsed'].dt.isocalendar().week.astype('Int64')\n",
        "\n",
        "# Is it a holiday season release? (Nov-Dec or Summer Jun-Aug)\n",
        "df['is_holiday_release'] = df['release_month'].isin([11, 12]).astype(int)\n",
        "df['is_summer_release'] = df['release_month'].isin([6, 7, 8]).astype(int)\n",
        "df['is_weekend_release'] = df['release_dayofweek'].isin([4, 5, 6]).astype(int)  # Fri, Sat, Sun\n",
        "\n",
        "print(\"‚úÖ Temporal features extracted\")\n",
        "\n",
        "# 2.3 Merge with Director information\n",
        "directors_df_renamed = directors_df.rename(columns={\n",
        "    'director_name': 'director',\n",
        "    'total_movies': 'director_total_movies',\n",
        "    'avg_total_gross': 'director_avg_total_gross',\n",
        "    'avg_opening_revenue': 'director_avg_opening_revenue',\n",
        "    'avg_final_budget': 'director_avg_budget',\n",
        "    'avg_tmdb_rating': 'director_avg_rating'\n",
        "})\n",
        "\n",
        "df = df.merge(directors_df_renamed[['director', 'director_id', 'director_total_movies',\n",
        "                                     'director_avg_total_gross', 'director_avg_opening_revenue',\n",
        "                                     'director_avg_budget', 'director_avg_rating']],\n",
        "              on='director', how='left')\n",
        "\n",
        "print(\"‚úÖ Director features merged\")\n",
        "\n",
        "# 2.4 Merge with Distributor information\n",
        "distributors_df_renamed = distributors_df.rename(columns={\n",
        "    'distributor_name': 'distributor',\n",
        "    'total_movies': 'distributor_total_movies',\n",
        "    'total_gross': 'distributor_total_gross',\n",
        "    'avg_opening_revenue': 'distributor_avg_opening_revenue',\n",
        "    'avg_final_budget': 'distributor_avg_budget',\n",
        "    'market_share': 'distributor_market_share'\n",
        "})\n",
        "\n",
        "df = df.merge(distributors_df_renamed[['distributor', 'distributor_id', 'distributor_total_movies',\n",
        "                                        'distributor_total_gross', 'distributor_avg_opening_revenue',\n",
        "                                        'distributor_avg_budget', 'distributor_market_share']],\n",
        "              on='distributor', how='left')\n",
        "\n",
        "print(\"‚úÖ Distributor features merged\")\n",
        "\n",
        "# 2.5 Create additional features\n",
        "# Budget-to-rating ratio\n",
        "df['budget_per_rating'] = df['final_budget'] / (df['tmdb_rating'] + 0.1)\n",
        "\n",
        "# Genre counts\n",
        "genre_cols = [col for col in df.columns if col.startswith('genre_')]\n",
        "df['genre_count'] = df[genre_cols].sum(axis=1)\n",
        "\n",
        "# Is it a major studio?\n",
        "major_studios = ['Warner Bros.', 'Paramount Pictures', 'Sony Pictures Releasing',\n",
        "                 'Walt Disney Studios Motion Pictures', 'Universal Pictures',\n",
        "                 'Twentieth Century Fox', '20th Century Studios', 'Lionsgate']\n",
        "df['is_major_studio'] = df['distributor'].isin(major_studios).astype(int)\n",
        "\n",
        "# Director experience level\n",
        "df['director_experience'] = pd.cut(df['director_total_movies'].fillna(1),\n",
        "                                   bins=[0, 2, 5, 10, 100],\n",
        "                                   labels=[1, 2, 3, 4]).astype(float)\n",
        "\n",
        "print(\"‚úÖ Additional features created\")\n",
        "\n",
        "# 2.6 Handle missing values\n",
        "print(f\"\\nüìä Missing values before imputation:\")\n",
        "missing_cols = df.isnull().sum()\n",
        "print(missing_cols[missing_cols > 0])\n",
        "\n",
        "# Fill missing values with median for numerical columns\n",
        "numerical_cols_to_fill = ['director_total_movies', 'director_avg_total_gross',\n",
        "                          'director_avg_opening_revenue', 'director_avg_budget',\n",
        "                          'director_avg_rating', 'distributor_total_movies',\n",
        "                          'distributor_total_gross', 'distributor_avg_opening_revenue',\n",
        "                          'distributor_avg_budget', 'distributor_market_share',\n",
        "                          'release_month', 'release_day', 'release_dayofweek',\n",
        "                          'release_quarter', 'release_weekofyear', 'final_budget',\n",
        "                          'runtime', 'tmdb_rating', 'tmdb_votes']\n",
        "\n",
        "for col in numerical_cols_to_fill:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# Fill categorical with mode\n",
        "df['director_id'] = df['director_id'].fillna(-1)\n",
        "df['distributor_id'] = df['distributor_id'].fillna(-1)\n",
        "df['director_experience'] = df['director_experience'].fillna(1)\n",
        "\n",
        "print(\"‚úÖ Missing values handled\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM2buguvqAop",
        "outputId": "8c4d8236-4185-464e-f5be-621f7dbb7c1c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 2: DATA PREPROCESSING & FEATURE ENGINEERING\n",
            "================================================================================\n",
            "‚úÖ Temporal features extracted\n",
            "‚úÖ Director features merged\n",
            "‚úÖ Distributor features merged\n",
            "‚úÖ Additional features created\n",
            "\n",
            "üìä Missing values before imputation:\n",
            "distributor                        17\n",
            "imdb_id                            10\n",
            "imdb_rating                        12\n",
            "director                            8\n",
            "genres                              3\n",
            "director_id                         8\n",
            "director_total_movies               8\n",
            "director_avg_total_gross            8\n",
            "director_avg_opening_revenue        8\n",
            "director_avg_budget                 8\n",
            "director_avg_rating                 8\n",
            "distributor_id                     17\n",
            "distributor_total_movies           17\n",
            "distributor_total_gross            17\n",
            "distributor_avg_opening_revenue    17\n",
            "distributor_avg_budget             17\n",
            "distributor_market_share           17\n",
            "dtype: int64\n",
            "‚úÖ Missing values handled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a44e163"
      },
      "source": [
        "### Step 3: Prepare Features for Modeling\n",
        "\n",
        "This cell defines and prepares the feature set (`X`) and the target variable (`y`) for the machine learning models. It performs the following actions:\n",
        "\n",
        "1.  **Feature Column Selection:** A comprehensive list of `feature_columns` is explicitly defined, including core movie details, temporal attributes, genre indicators, and the merged director and distributor statistics, along with the newly engineered features.\n",
        "2.  **Target Variable Definition:** The `opening_revenue` column is designated as the `target` variable.\n",
        "3.  **Data Filtering:** The DataFrame is filtered to include only rows where the `opening_revenue` is not missing and is greater than zero, ensuring a clean target for prediction.\n",
        "4.  **Feature Validation:** Checks if all specified `feature_columns` are present in the filtered DataFrame and reports any discrepancies.\n",
        "5.  **Dataset Separation:** The filtered DataFrame is split into `X` (features) and `y` (target).\n",
        "6.  **Final NaN Handling:** Any remaining `NaN` values in the feature matrix `X` are filled with `0`.\n",
        "7.  **Target Statistics Display:** Basic statistical summaries (mean, median, standard deviation, min, max) of the `opening_revenue` are printed to understand its distribution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 3: PREPARE FEATURES FOR MODELING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: PREPARE FEATURES FOR MODELING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select features for modeling\n",
        "feature_columns = [\n",
        "    # Core movie features\n",
        "    'year', 'final_budget', 'tmdb_rating', 'tmdb_votes', 'runtime',\n",
        "\n",
        "    # Temporal features\n",
        "    'release_month', 'release_day', 'release_dayofweek', 'release_quarter',\n",
        "    'is_holiday_release', 'is_summer_release', 'is_weekend_release',\n",
        "\n",
        "    # Genre features\n",
        "    'genre_Drama', 'genre_Comedy', 'genre_Action', 'genre_Adventure',\n",
        "    'genre_Thriller', 'genre_Science Fiction', 'genre_Other', 'genre_count',\n",
        "\n",
        "    # Director features\n",
        "    'director_id', 'director_total_movies', 'director_avg_total_gross',\n",
        "    'director_avg_opening_revenue', 'director_avg_budget', 'director_avg_rating',\n",
        "    'director_experience',\n",
        "\n",
        "    # Distributor features\n",
        "    'distributor_id', 'distributor_total_movies', 'distributor_total_gross',\n",
        "    'distributor_avg_opening_revenue', 'distributor_avg_budget',\n",
        "    'distributor_market_share', 'is_major_studio',\n",
        "\n",
        "    # Engineered features\n",
        "    'budget_per_rating'\n",
        "]\n",
        "\n",
        "# Target variable\n",
        "target = 'opening_revenue'\n",
        "\n",
        "# Filter rows with valid target\n",
        "df_model = df[df[target].notna() & (df[target] > 0)].copy()\n",
        "\n",
        "# Verify all feature columns exist\n",
        "available_features = [col for col in feature_columns if col in df_model.columns]\n",
        "missing_features = [col for col in feature_columns if col not in df_model.columns]\n",
        "\n",
        "if missing_features:\n",
        "    print(f\"‚ö†Ô∏è Missing features (will be skipped): {missing_features}\")\n",
        "\n",
        "print(f\"‚úÖ Using {len(available_features)} features for modeling\")\n",
        "\n",
        "# Prepare X and y\n",
        "X = df_model[available_features].copy()\n",
        "y = df_model[target].copy()\n",
        "\n",
        "# Handle any remaining NaN values\n",
        "X = X.fillna(0)\n",
        "\n",
        "print(f\"\\nüìä Dataset shape: X={X.shape}, y={y.shape}\")\n",
        "print(f\"üìä Target (opening_revenue) statistics:\")\n",
        "print(f\"   Mean: ${y.mean():,.0f}\")\n",
        "print(f\"   Median: ${y.median():,.0f}\")\n",
        "print(f\"   Std: ${y.std():,.0f}\")\n",
        "print(f\"   Min: ${y.min():,.0f}\")\n",
        "print(f\"   Max: ${y.max():,.0f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JQUklH9qF0I",
        "outputId": "9fcd8223-af50-415d-f6d1-21f1c6c75c23"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 3: PREPARE FEATURES FOR MODELING\n",
            "================================================================================\n",
            "‚úÖ Using 35 features for modeling\n",
            "\n",
            "üìä Dataset shape: X=(3332, 35), y=(3332,)\n",
            "üìä Target (opening_revenue) statistics:\n",
            "   Mean: $21,262,104\n",
            "   Median: $7,866,504\n",
            "   Std: $39,883,720\n",
            "   Min: $962\n",
            "   Max: $473,894,638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4f93c47"
      },
      "source": [
        "### Log Transformation for Skewed Target\n",
        "\n",
        "This is a crucial preprocessing step for the target variable, `opening_revenue`, which typically exhibits a highly skewed distribution (many low values, few very high values). Applying a logarithmic transformation helps to:\n",
        "\n",
        "*   **Reduce Skewness:** Makes the distribution more symmetric, bringing it closer to a normal distribution, which is often an assumption for many regression models.\n",
        "*   **Stabilize Variance:** Can help in making the variance more consistent across the range of the target variable.\n",
        "*   **Improve Model Performance:** Models often perform better when trained on transformed targets with more normalized distributions.\n",
        "\n",
        "The `np.log1p(y)` function is used, which calculates `log(1 + y)`. This is preferred over `log(y)` because it gracefully handles zero values (where `log(0)` is undefined) and behaves similarly to `log(y)` for larger values. The cell outputs the skewness before and after the transformation to demonstrate its effect."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CRITICAL: LOG TRANSFORMATION FOR SKEWED TARGET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"LOG TRANSFORMATION (CRITICAL FOR REVENUE DATA)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Apply log1p transformation to handle skewed revenue distribution\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "print(f\"üìä Original Target - Skewness: {y.skew():.2f}\")\n",
        "print(f\"üìä Log-transformed Target - Skewness: {y_log.skew():.2f}\")\n",
        "print(\"‚úÖ Log transformation applied to reduce skewness and improve model performance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y2ORyl5qKsp",
        "outputId": "51ad77b1-aced-410c-ea5c-45a3a97ce98f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOG TRANSFORMATION (CRITICAL FOR REVENUE DATA)\n",
            "================================================================================\n",
            "üìä Original Target - Skewness: 4.35\n",
            "üìä Log-transformed Target - Skewness: -0.53\n",
            "‚úÖ Log transformation applied to reduce skewness and improve model performance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a95599a6"
      },
      "source": [
        "### Step 4: Train-Test Split\n",
        "\n",
        "This cell divides the dataset into training and testing sets, a standard practice to evaluate a machine learning model's performance on unseen data. Here's a breakdown:\n",
        "\n",
        "1.  **Splitting Data:** The `train_test_split` function from `sklearn.model_selection` is used to partition the features (`X`) and the log-transformed target (`y_log`) into training (`X_train`, `y_train_log`) and testing (`X_test`, `y_test_log`) subsets. A `test_size` of `0.2` means 20% of the data is reserved for testing, and `random_state=42` ensures that the split is reproducible.\n",
        "2.  **Original Scale Target Variables:** Separate variables `y_train_orig` and `y_test_orig` are created by applying `np.expm1` (the inverse of `np.log1p`) to the log-transformed target variables. These original-scale target variables are essential for calculating evaluation metrics (like MAE and RMSE) in terms of actual dollar amounts, making them directly interpretable.\n",
        "\n",
        "The cell concludes by printing the number of samples in both the training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 4: TRAIN-TEST SPLIT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: TRAIN-TEST SPLIT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Split using log-transformed target\n",
        "X_train, X_test, y_train_log, y_test_log = train_test_split(\n",
        "    X, y_log, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Keep original scale for evaluation\n",
        "y_train_orig = np.expm1(y_train_log)\n",
        "y_test_orig = np.expm1(y_test_log)\n",
        "\n",
        "print(f\"‚úÖ Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"‚úÖ Test set: {X_test.shape[0]} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ2uMkeMqOa8",
        "outputId": "7f101813-f8f5-4d3f-8723-0e354cd454f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 4: TRAIN-TEST SPLIT\n",
            "================================================================================\n",
            "‚úÖ Training set: 2665 samples\n",
            "‚úÖ Test set: 667 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dfc6498"
      },
      "source": [
        "### Step 5: Define and Compare Multiple Models\n",
        "\n",
        "This extensive cell defines, trains, and evaluates several machine learning models, including an ensemble stacking model, to find the best performer for predicting movie opening revenue.\n",
        "\n",
        "1.  **`evaluate_model_detailed` Function:** A helper function is defined to streamline the evaluation process. It trains a given model on the log-transformed training data, makes predictions, and then converts both actuals and predictions back to the original scale to calculate key metrics: RMSE, MAE, R¬≤, and MAPE (Mean Absolute Percentage Error).\n",
        "2.  **Individual Model Definitions:** Five different regression models are instantiated with pre-tuned hyperparameters:\n",
        "    *   **XGBoost (`xgb.XGBRegressor`):** A highly efficient and robust gradient boosting algorithm.\n",
        "    *   **LightGBM (`lgb.LGBMRegressor`):** Another fast, distributed, high-performance gradient boosting framework.\n",
        "    *   **CatBoost (`CatBoostRegressor`):** A gradient boosting library known for its handling of categorical features and robustness.\n",
        "    *   **Random Forest (`RandomForestRegressor`):** An ensemble method that operates by constructing multiple decision trees during training.\n",
        "    *   **Gradient Boosting (`GradientBoostingRegressor`):** A powerful ensemble technique that builds models sequentially, each one trying to correct the errors of the previous one.\n",
        "3.  **Model Training and Evaluation Loop:** Each model is trained on the `X_train` and `y_train_log` data, and its performance is evaluated on the test set. The `Test R¬≤` and `Test MAE` are printed for each model.\n",
        "\n",
        "### Step 5.5: Create Ensemble (Stacking) Model\n",
        "\n",
        "1.  **Base Estimators:** A set of `base_estimators` (XGBoost, LightGBM, CatBoost, Random Forest) is defined. These models will form the first layer of the stacking ensemble.\n",
        "2.  **Stacking Regressor:** A `StackingRegressor` is created, which combines the predictions of the `base_estimators` using a `Ridge` regression as the `final_estimator` (meta-learner). This approach leverages the strengths of multiple models.\n",
        "3.  **Ensemble Training:** The stacking model is then trained and evaluated, and its performance metrics are added to the results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 5: DEFINE AND COMPARE MULTIPLE MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: COMPARING MULTIPLE ML MODELS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate_model_detailed(model, X_train, X_test, y_train_log, y_test_log, model_name):\n",
        "    \"\"\"Train and evaluate model with both log and original scale metrics\"\"\"\n",
        "\n",
        "    # Train\n",
        "    model.fit(X_train, y_train_log)\n",
        "\n",
        "    # Predict (log scale)\n",
        "    y_pred_train_log = model.predict(X_train)\n",
        "    y_pred_test_log = model.predict(X_test)\n",
        "\n",
        "    # Convert back to original scale\n",
        "    y_pred_train = np.expm1(y_pred_train_log)\n",
        "    y_pred_test = np.expm1(y_pred_test_log)\n",
        "    y_train_orig = np.expm1(y_train_log)\n",
        "    y_test_orig = np.expm1(y_test_log)\n",
        "\n",
        "    # Calculate metrics on original scale\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred_test))\n",
        "    train_mae = mean_absolute_error(y_train_orig, y_pred_train)\n",
        "    test_mae = mean_absolute_error(y_test_orig, y_pred_test)\n",
        "    train_r2 = r2_score(y_train_orig, y_pred_train)\n",
        "    test_r2 = r2_score(y_test_orig, y_pred_test)\n",
        "\n",
        "    # MAPE (avoiding division by zero)\n",
        "    train_mape = np.mean(np.abs((y_train_orig - y_pred_train) / (y_train_orig + 1))) * 100\n",
        "    test_mape = np.mean(np.abs((y_test_orig - y_pred_test) / (y_test_orig + 1))) * 100\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'model': model,\n",
        "        'train_rmse': train_rmse,\n",
        "        'test_rmse': test_rmse,\n",
        "        'train_mae': train_mae,\n",
        "        'test_mae': test_mae,\n",
        "        'train_r2': train_r2,\n",
        "        'test_r2': test_r2,\n",
        "        'train_mape': train_mape,\n",
        "        'test_mape': test_mape,\n",
        "        'y_pred_test': y_pred_test,\n",
        "        'y_pred_test_log': y_pred_test_log\n",
        "    }\n",
        "\n",
        "# Define models with optimized parameters (less overfitting)\n",
        "models = {\n",
        "    'XGBoost': xgb.XGBRegressor(\n",
        "        objective='reg:squarederror',\n",
        "        n_estimators=300,\n",
        "        max_depth=4,  # Reduced from 6\n",
        "        learning_rate=0.03,\n",
        "        subsample=0.7,\n",
        "        colsample_bytree=0.7,\n",
        "        min_child_weight=5,\n",
        "        gamma=0.2,\n",
        "        reg_alpha=0.5,\n",
        "        reg_lambda=2.0,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "\n",
        "    'LightGBM': lgb.LGBMRegressor(\n",
        "        objective='regression',\n",
        "        n_estimators=300,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.03,\n",
        "        subsample=0.7,\n",
        "        colsample_bytree=0.7,\n",
        "        min_child_samples=20,\n",
        "        reg_alpha=0.5,\n",
        "        reg_lambda=2.0,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbose=-1\n",
        "    ),\n",
        "\n",
        "    'CatBoost': CatBoostRegressor(\n",
        "        iterations=300,\n",
        "        depth=4,\n",
        "        learning_rate=0.03,\n",
        "        l2_leaf_reg=5,\n",
        "        random_seed=42,\n",
        "        verbose=0\n",
        "    ),\n",
        "\n",
        "    'Random Forest': RandomForestRegressor(\n",
        "        n_estimators=300,\n",
        "        max_depth=8,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=5,\n",
        "        max_features='sqrt',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "\n",
        "    'Gradient Boosting': GradientBoostingRegressor(\n",
        "        n_estimators=300,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.03,\n",
        "        subsample=0.7,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=5,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "# Train and evaluate all models\n",
        "print(\"\\nüîÑ Training and evaluating models...\")\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n   Training {name}...\")\n",
        "    results[name] = evaluate_model_detailed(\n",
        "        model, X_train, X_test, y_train_log, y_test_log, name\n",
        "    )\n",
        "    print(f\"   ‚úÖ {name}: Test R¬≤ = {results[name]['test_r2']:.4f}, Test MAE = ${results[name]['test_mae']:,.0f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5.5: CREATE ENSEMBLE (STACKING) MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5.5: BUILDING ENSEMBLE (STACKING) MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define base estimators for stacking\n",
        "base_estimators = [\n",
        "    ('xgb', xgb.XGBRegressor(\n",
        "        n_estimators=200, max_depth=4, learning_rate=0.03,\n",
        "        subsample=0.7, colsample_bytree=0.7, random_state=42, n_jobs=-1\n",
        "    )),\n",
        "    ('lgb', lgb.LGBMRegressor(\n",
        "        n_estimators=200, max_depth=4, learning_rate=0.03,\n",
        "        subsample=0.7, colsample_bytree=0.7, random_state=42, n_jobs=-1, verbose=-1\n",
        "    )),\n",
        "    ('cat', CatBoostRegressor(\n",
        "        iterations=200, depth=4, learning_rate=0.03,\n",
        "        random_seed=42, verbose=0\n",
        "    )),\n",
        "    ('rf', RandomForestRegressor(\n",
        "        n_estimators=200, max_depth=6, min_samples_leaf=5,\n",
        "        random_state=42, n_jobs=-1\n",
        "    ))\n",
        "]\n",
        "\n",
        "# Create stacking regressor with Ridge as final estimator\n",
        "stacking_model = StackingRegressor(\n",
        "    estimators=base_estimators,\n",
        "    final_estimator=Ridge(alpha=1.0),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"üîÑ Training Stacking Ensemble (this may take a moment)...\")\n",
        "results['Stacking Ensemble'] = evaluate_model_detailed(\n",
        "    stacking_model, X_train, X_test, y_train_log, y_test_log, 'Stacking Ensemble'\n",
        ")\n",
        "print(f\"‚úÖ Stacking Ensemble: Test R¬≤ = {results['Stacking Ensemble']['test_r2']:.4f}, Test MAE = ${results['Stacking Ensemble']['test_mae']:,.0f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcbZHoeBqc8o",
        "outputId": "8e0e492e-bb21-46a3-82be-adf648090722"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 5: COMPARING MULTIPLE ML MODELS\n",
            "================================================================================\n",
            "\n",
            "üîÑ Training and evaluating models...\n",
            "\n",
            "   Training XGBoost...\n",
            "   ‚úÖ XGBoost: Test R¬≤ = 0.6879, Test MAE = $8,852,665\n",
            "\n",
            "   Training LightGBM...\n",
            "   ‚úÖ LightGBM: Test R¬≤ = 0.6906, Test MAE = $8,996,675\n",
            "\n",
            "   Training CatBoost...\n",
            "   ‚úÖ CatBoost: Test R¬≤ = 0.5536, Test MAE = $10,272,286\n",
            "\n",
            "   Training Random Forest...\n",
            "   ‚úÖ Random Forest: Test R¬≤ = 0.5398, Test MAE = $10,815,545\n",
            "\n",
            "   Training Gradient Boosting...\n",
            "   ‚úÖ Gradient Boosting: Test R¬≤ = 0.7286, Test MAE = $8,350,308\n",
            "\n",
            "================================================================================\n",
            "STEP 5.5: BUILDING ENSEMBLE (STACKING) MODEL\n",
            "================================================================================\n",
            "üîÑ Training Stacking Ensemble (this may take a moment)...\n",
            "‚úÖ Stacking Ensemble: Test R¬≤ = 0.7133, Test MAE = $8,712,309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5476c22"
      },
      "source": [
        "### Step 6: Model Comparison and Selection\n",
        "\n",
        "This cell compares the performance of all trained models and selects the single best model based on its test set metrics.\n",
        "\n",
        "1.  **Comparison DataFrame:** A pandas DataFrame, `comparison_df`, is created to aggregate the performance metrics (Train R¬≤, Test R¬≤, Train MAE, Test MAE, Test MAPE, and an Overfit Ratio) for all models.\n",
        "2.  **Sorted Output:** The `comparison_df` is sorted by `Test R¬≤` in descending order and printed in a clean, formatted table. This allows for a quick and clear comparison of how well each model generalizes to unseen data.\n",
        "3.  **Best Model Identification:** The model with the highest `Test R¬≤` score is identified as the `best_model`. Its name, the model object itself, and its performance metrics are stored in `best_model_name`, `best_model`, and `best_metrics` variables respectively.\n",
        "4.  **Cross-Validation:** To further validate the `best_model`'s robustness, 5-fold cross-validation is performed on the entire dataset (`X` and `y_log`). The mean and standard deviation of the cross-validation R¬≤ scores are printed, providing a more reliable estimate of the model's generalization ability.\n",
        "5.  **Storing Test Metrics:** The test predictions (`y_pred_test`) from the best model and its key performance metrics (`test_metrics`) are stored for use in subsequent steps (e.g., visualizations and final summary)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 6: MODEL COMPARISON AND SELECTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: MODEL COMPARISON AND EVALUATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame([\n",
        "    {\n",
        "        'Model': name,\n",
        "        'Train R¬≤': res['train_r2'],\n",
        "        'Test R¬≤': res['test_r2'],\n",
        "        'Train MAE': res['train_mae'],\n",
        "        'Test MAE': res['test_mae'],\n",
        "        'Train RMSE': res['train_rmse'],\n",
        "        'Test RMSE': res['test_rmse'],\n",
        "        'Train MAPE': res['train_mape'],\n",
        "        'Test MAPE': res['test_mape'],\n",
        "        'Overfit Ratio': res['train_r2'] / max(res['test_r2'], 0.001)\n",
        "    }\n",
        "    for name, res in results.items()\n",
        "]).sort_values('Test R¬≤', ascending=False)\n",
        "\n",
        "print(\"\\nüìä MODEL COMPARISON (Sorted by Test R¬≤):\")\n",
        "print(\"=\" * 120)\n",
        "print(f\"{'Model':<20} {'Train R¬≤':>10} {'Test R¬≤':>10} {'Train MAE':>15} {'Test MAE':>15} {'Test MAPE':>12} {'Overfit':>10}\")\n",
        "print(\"-\" * 120)\n",
        "\n",
        "for _, row in comparison_df.iterrows():\n",
        "    print(f\"{row['Model']:<20} {row['Train R¬≤']:>10.4f} {row['Test R¬≤']:>10.4f} \"\n",
        "          f\"${row['Train MAE']:>13,.0f} ${row['Test MAE']:>13,.0f} \"\n",
        "          f\"{row['Test MAPE']:>10.2f}% {row['Overfit Ratio']:>9.2f}x\")\n",
        "\n",
        "# Select best model based on Test R¬≤ (or you can use Test MAE)\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_model = results[best_model_name]['model']\n",
        "best_metrics = results[best_model_name]\n",
        "\n",
        "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
        "print(f\"   ‚Ä¢ Test R¬≤: {best_metrics['test_r2']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Test MAE: ${best_metrics['test_mae']:,.0f}\")\n",
        "print(f\"   ‚Ä¢ Test RMSE: ${best_metrics['test_rmse']:,.0f}\")\n",
        "print(f\"   ‚Ä¢ Test MAPE: {best_metrics['test_mape']:.2f}%\")\n",
        "\n",
        "# Cross-validation for best model\n",
        "print(f\"\\nüìä 5-Fold Cross-Validation for {best_model_name}:\")\n",
        "cv_scores = cross_val_score(best_model, X, y_log, cv=5, scoring='r2')\n",
        "print(f\"   CV R¬≤ Scores: {cv_scores}\")\n",
        "print(f\"   Mean CV R¬≤: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "\n",
        "# Store test metrics for later use\n",
        "y_pred_test = best_metrics['y_pred_test']\n",
        "test_metrics = {\n",
        "    'r2': best_metrics['test_r2'],\n",
        "    'mae': best_metrics['test_mae'],\n",
        "    'rmse': best_metrics['test_rmse'],\n",
        "    'mape': best_metrics['test_mape']\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xghVid6zqiFz",
        "outputId": "66455eb8-de0b-4817-83b1-0d1af19537ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 6: MODEL COMPARISON AND EVALUATION\n",
            "================================================================================\n",
            "\n",
            "üìä MODEL COMPARISON (Sorted by Test R¬≤):\n",
            "========================================================================================================================\n",
            "Model                  Train R¬≤    Test R¬≤       Train MAE        Test MAE    Test MAPE    Overfit\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Gradient Boosting        0.8177     0.7286 $    6,615,083 $    8,350,308     290.68%      1.12x\n",
            "Stacking Ensemble        0.7557     0.7133 $    7,367,392 $    8,712,309     301.45%      1.06x\n",
            "LightGBM                 0.7421     0.6906 $    7,427,679 $    8,996,675     299.34%      1.07x\n",
            "XGBoost                  0.7577     0.6879 $    7,238,218 $    8,852,665     302.92%      1.10x\n",
            "CatBoost                 0.5686     0.5536 $    9,727,191 $   10,272,286     386.74%      1.03x\n",
            "Random Forest            0.5953     0.5398 $    9,550,550 $   10,815,545     325.91%      1.10x\n",
            "\n",
            "üèÜ BEST MODEL: Gradient Boosting\n",
            "   ‚Ä¢ Test R¬≤: 0.7286\n",
            "   ‚Ä¢ Test MAE: $8,350,308\n",
            "   ‚Ä¢ Test RMSE: $20,702,441\n",
            "   ‚Ä¢ Test MAPE: 290.68%\n",
            "\n",
            "üìä 5-Fold Cross-Validation for Gradient Boosting:\n",
            "   CV R¬≤ Scores: [0.72321146 0.79455561 0.81909241 0.80130076 0.81767567]\n",
            "   Mean CV R¬≤: 0.7912 (+/- 0.0353)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37b5ce94"
      },
      "source": [
        "### Step 7: Feature Importance Analysis\n",
        "\n",
        "This cell identifies and visualizes the most influential features that the `best_model` used to make its predictions. Understanding feature importance can provide valuable insights into the data and the model's decision-making process.\n",
        "\n",
        "1.  **Retrieve Importances:** The code dynamically retrieves feature importances from the `best_model`. For tree-based models (like Gradient Boosting, XGBoost), it directly accesses the `feature_importances_` attribute. If the `best_model` is a `StackingRegressor` (which doesn't have a single `feature_importances_`), it defaults to using the importances from the XGBoost model within the ensemble, as XGBoost usually provides good feature importance scores.\n",
        "2.  **Create DataFrame:** The feature names and their corresponding importance scores are compiled into a pandas DataFrame and sorted in descending order of importance.\n",
        "3.  **Display Top Features:** The top 15 most important features are printed to the console.\n",
        "4.  **Visualize Importances:** A horizontal bar chart is generated to graphically represent the top 20 features and their importance scores. This plot is saved as `feature_importance.png`, offering a clear visual summary of which features contributed most to the model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 7: FEATURE IMPORTANCE ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get feature importance (handle different model types)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    importances = best_model.feature_importances_\n",
        "elif hasattr(best_model, 'estimators_'):\n",
        "    # For stacking, use XGBoost importance\n",
        "    importances = results['XGBoost']['model'].feature_importances_\n",
        "else:\n",
        "    importances = np.zeros(len(available_features))\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': available_features,\n",
        "    'importance': importances\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nüîù Top 15 Most Important Features:\")\n",
        "print(feature_importance.head(15).to_string(index=False))\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = feature_importance.head(20)\n",
        "plt.barh(range(len(top_features)), top_features['importance'].values)\n",
        "plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title(f'Top 20 Feature Importances - {best_model_name}')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"\\n‚úÖ Feature importance plot saved as 'feature_importance.png'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8XpVSHEq-Jr",
        "outputId": "42510aaf-ac09-4deb-ee74-31eb821af9df"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 7: FEATURE IMPORTANCE ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "üîù Top 15 Most Important Features:\n",
            "                        feature  importance\n",
            "   director_avg_opening_revenue    0.587761\n",
            "distributor_avg_opening_revenue    0.177599\n",
            "              budget_per_rating    0.031471\n",
            "            director_avg_budget    0.026602\n",
            "                     tmdb_votes    0.023803\n",
            "                   final_budget    0.022849\n",
            "                  release_month    0.013410\n",
            "                    director_id    0.013108\n",
            "          director_total_movies    0.012872\n",
            "              release_dayofweek    0.012706\n",
            "                           year    0.012303\n",
            "             is_weekend_release    0.008308\n",
            "                    tmdb_rating    0.007767\n",
            "       director_avg_total_gross    0.007458\n",
            "            director_avg_rating    0.007012\n",
            "\n",
            "‚úÖ Feature importance plot saved as 'feature_importance.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d919e762"
      },
      "source": [
        "### Step 8: Prediction for 'Avatar: Fire and Ash'\n",
        "\n",
        "This pivotal cell uses the trained `best_model` to predict the opening revenue for the highly anticipated movie 'Avatar: Fire and Ash' and provides extensive context for this prediction.\n",
        "\n",
        "1.  **Define Hypothetical Movie Features:** A dictionary `avatar_features` is created to encapsulate all relevant information for 'Avatar: Fire and Ash', including its estimated budget, release date, director, distributor, and genre.\n",
        "2.  **Retrieve Historical Stats:** It dynamically fetches historical performance data for the director (James Cameron) and distributor (Walt Disney Studios Motion Pictures) from the pre-loaded dimension tables (`directors_df`, `distributors_df`). These aggregated statistics are crucial inputs for the model.\n",
        "3.  **Construct Feature Vector:** A `avatar_feature_vector` is carefully assembled. This dictionary contains all the features needed by the model, matching the `available_features` used during training. It includes temporal features derived from the release date and engineered features like `budget_per_rating`.\n",
        "4.  **Create Prediction DataFrame:** A pandas DataFrame, `avatar_df`, is created from the feature vector, ensuring its structure (columns and order) is identical to the training data.\n",
        "5.  **Generate Prediction:** The `best_model` makes a prediction on `avatar_df`. The output is initially in log scale (`avatar_pred_log`) and is then transformed back to the original dollar amount using `np.expm1` to get `avatar_prediction`.\n",
        "6.  **Ensemble Predictions:** Predictions from all individual models (XGBoost, LightGBM, CatBoost, Random Forest, Gradient Boosting, and Stacking Ensemble) are also generated and displayed. An `ensemble_avg` is calculated, offering a potentially more robust prediction by combining multiple model outputs.\n",
        "7.  **Display Results and Context:** The predicted opening revenue from the `best_model` and the `ensemble_avg` are prominently shown. Additional context, such as the release date, budget, and the average historical performance of the director and distributor, is provided.\n",
        "8.  **Franchise Comparison:** The prediction is benchmarked against the historical opening revenues of previous 'Avatar' movies found in the original `movies_df`.\n",
        "9.  **95% Prediction Interval:** A 95% prediction interval is calculated based on the standard deviation of the residuals from the test set. This interval provides a range of values within which the actual opening revenue is likely to fall, giving an estimate of the prediction's uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 8: PREDICTION FOR AVATAR: FIRE AND ASH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 8: PREDICTING OPENING REVENUE FOR 'AVATAR: FIRE AND ASH'\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Avatar: Fire and Ash features\n",
        "avatar_features = {\n",
        "    'title': 'Avatar: Fire and Ash',\n",
        "    'year': 2025,\n",
        "    'release_date': '2025-12-19',  # Friday before Christmas\n",
        "    'budget': 350_000_000,\n",
        "    'is_franchise': 1,\n",
        "    'franchise_name': 'Avatar Collection',\n",
        "    'director': 'James Cameron',\n",
        "    'distributor': 'Walt Disney Studios Motion Pictures',\n",
        "    'genres': 'Action, Adventure, Science Fiction',\n",
        "    'original_language': 'en',\n",
        "    'popularity': 500,\n",
        "    'tmdb_rating': 8.0,\n",
        "    'tmdb_votes': 50000,\n",
        "    'vote_average': 8.0,\n",
        "    'vote_count': 50000,\n",
        "    'rating': 'PG-13',\n",
        "    'total_gross': 700_000_000,\n",
        "    'opening_week': 180_000_000,\n",
        "    'runtime': 190  # Estimated runtime similar to Way of Water\n",
        "}\n",
        "\n",
        "# Get James Cameron's director stats\n",
        "james_cameron_stats = directors_df[directors_df['director_name'] == 'James Cameron'].iloc[0]\n",
        "print(f\"\\nüé¨ James Cameron Stats:\")\n",
        "print(f\"   Total Movies: {james_cameron_stats['total_movies']}\")\n",
        "print(f\"   Avg Total Gross: ${james_cameron_stats['avg_total_gross']:,.0f}\")\n",
        "print(f\"   Avg Opening Revenue: ${james_cameron_stats['avg_opening_revenue']:,.0f}\")\n",
        "print(f\"   Avg Budget: ${james_cameron_stats['avg_final_budget']:,.0f}\")\n",
        "print(f\"   Avg TMDB Rating: {james_cameron_stats['avg_tmdb_rating']:.2f}\")\n",
        "\n",
        "# Get Walt Disney distributor stats\n",
        "disney_stats = distributors_df[distributors_df['distributor_name'] == 'Walt Disney Studios Motion Pictures'].iloc[0]\n",
        "print(f\"\\nüè¢ Walt Disney Studios Stats:\")\n",
        "print(f\"   Total Movies: {disney_stats['total_movies']}\")\n",
        "print(f\"   Total Gross: ${disney_stats['total_gross']:,.0f}\")\n",
        "print(f\"   Avg Opening Revenue: ${disney_stats['avg_opening_revenue']:,.0f}\")\n",
        "print(f\"   Market Share: {disney_stats['market_share']}%\")\n",
        "\n",
        "# Parse release date for Avatar\n",
        "avatar_release_date = datetime.strptime(avatar_features['release_date'], '%Y-%m-%d')\n",
        "\n",
        "# Prepare feature vector for Avatar\n",
        "avatar_feature_vector = {\n",
        "    # Core features\n",
        "    'year': 2025,\n",
        "    'final_budget': 350_000_000,\n",
        "    'tmdb_rating': 8.0,\n",
        "    'tmdb_votes': 50000,\n",
        "    'runtime': 190,\n",
        "\n",
        "    # Temporal features\n",
        "    'release_month': 12,\n",
        "    'release_day': 19,\n",
        "    'release_dayofweek': 4,  # Friday\n",
        "    'release_quarter': 4,\n",
        "    'is_holiday_release': 1,\n",
        "    'is_summer_release': 0,\n",
        "    'is_weekend_release': 1,\n",
        "\n",
        "    # Genre features (Action, Adventure, Science Fiction)\n",
        "    'genre_Drama': 0,\n",
        "    'genre_Comedy': 0,\n",
        "    'genre_Action': 1,\n",
        "    'genre_Adventure': 1,\n",
        "    'genre_Thriller': 0,\n",
        "    'genre_Science Fiction': 1,\n",
        "    'genre_Other': 0,\n",
        "    'genre_count': 3,\n",
        "\n",
        "    # Director features (James Cameron)\n",
        "    'director_id': james_cameron_stats['director_id'],\n",
        "    'director_total_movies': james_cameron_stats['total_movies'],\n",
        "    'director_avg_total_gross': james_cameron_stats['avg_total_gross'],\n",
        "    'director_avg_opening_revenue': james_cameron_stats['avg_opening_revenue'],\n",
        "    'director_avg_budget': james_cameron_stats['avg_final_budget'],\n",
        "    'director_avg_rating': james_cameron_stats['avg_tmdb_rating'],\n",
        "    'director_experience': 4,  # 10+ movies = experience level 4\n",
        "\n",
        "    # Distributor features (Walt Disney)\n",
        "    'distributor_id': disney_stats['distributor_id'],\n",
        "    'distributor_total_movies': disney_stats['total_movies'],\n",
        "    'distributor_total_gross': disney_stats['total_gross'],\n",
        "    'distributor_avg_opening_revenue': disney_stats['avg_opening_revenue'],\n",
        "    'distributor_avg_budget': disney_stats['avg_final_budget'],\n",
        "    'distributor_market_share': disney_stats['market_share'],\n",
        "    'is_major_studio': 1,\n",
        "\n",
        "    # Engineered features\n",
        "    'budget_per_rating': 350_000_000 / 8.1\n",
        "}\n",
        "\n",
        "# Create DataFrame with same columns as training data\n",
        "avatar_df = pd.DataFrame([avatar_feature_vector])\n",
        "\n",
        "# Ensure column order matches\n",
        "avatar_df = avatar_df[available_features]\n",
        "\n",
        "# Make prediction using log scale then convert back\n",
        "avatar_pred_log = best_model.predict(avatar_df)[0]\n",
        "avatar_prediction = np.expm1(avatar_pred_log)\n",
        "\n",
        "# Also get predictions from all models for comparison\n",
        "print(\"\\nüìä Predictions from all models:\")\n",
        "all_predictions = {}\n",
        "for name, res in results.items():\n",
        "    pred_log = res['model'].predict(avatar_df)[0]\n",
        "    pred = np.expm1(pred_log)\n",
        "    all_predictions[name] = pred\n",
        "    print(f\"   ‚Ä¢ {name}: ${pred:,.0f}\")\n",
        "\n",
        "# Use ensemble average for more robust prediction\n",
        "ensemble_avg = np.mean(list(all_predictions.values()))\n",
        "print(f\"\\n   üéØ Ensemble Average: ${ensemble_avg:,.0f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üé¨ PREDICTION RESULTS FOR 'AVATAR: FIRE AND ASH'\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nüéØ Predicted Opening Revenue ({best_model_name}): ${avatar_prediction:,.0f}\")\n",
        "print(f\"üéØ Ensemble Average Prediction: ${ensemble_avg:,.0f}\")\n",
        "print(f\"\\nüìä Prediction Context:\")\n",
        "print(f\"   ‚Ä¢ Release Date: December 19, 2025 (Friday, Holiday Season)\")\n",
        "print(f\"   ‚Ä¢ Budget: $350,000,000\")\n",
        "print(f\"   ‚Ä¢ Director: James Cameron (Avg Opening: ${james_cameron_stats['avg_opening_revenue']:,.0f})\")\n",
        "print(f\"   ‚Ä¢ Distributor: Walt Disney Studios (Avg Opening: ${disney_stats['avg_opening_revenue']:,.0f})\")\n",
        "\n",
        "# Compare with Avatar franchise history\n",
        "print(\"\\nüìà Avatar Franchise Comparison:\")\n",
        "avatar_movies = movies_df[movies_df['title'].str.contains('Avatar', case=False, na=False)]\n",
        "for _, movie in avatar_movies.iterrows():\n",
        "    print(f\"   ‚Ä¢ {movie['title']} ({movie['year']}): Opening = ${movie['opening_revenue']:,.0f}\")\n",
        "\n",
        "# Prediction intervals (using test residuals in log space then converting)\n",
        "y_pred_test_log = best_metrics['y_pred_test_log']\n",
        "residuals_log = y_test_log - y_pred_test_log\n",
        "residual_std_log = residuals_log.std()\n",
        "\n",
        "# Calculate interval in log space, then convert to original scale\n",
        "prediction_low_log = avatar_pred_log - 1.96 * residual_std_log\n",
        "prediction_high_log = avatar_pred_log + 1.96 * residual_std_log\n",
        "\n",
        "prediction_low = np.expm1(prediction_low_log)\n",
        "prediction_high = np.expm1(prediction_high_log)\n",
        "\n",
        "print(f\"\\nüìä 95% Prediction Interval:\")\n",
        "print(f\"   Lower Bound: ${max(0, prediction_low):,.0f}\")\n",
        "print(f\"   Upper Bound: ${prediction_high:,.0f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtyRAkQerJEF",
        "outputId": "9ca33e25-baf2-41a7-9380-2b2db4d2573a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 8: PREDICTING OPENING REVENUE FOR 'AVATAR: FIRE AND ASH'\n",
            "================================================================================\n",
            "\n",
            "üé¨ James Cameron Stats:\n",
            "   Total Movies: 8\n",
            "   Avg Total Gross: $372,014,938\n",
            "   Avg Opening Revenue: $88,431,457\n",
            "   Avg Budget: $256,000,000\n",
            "   Avg TMDB Rating: 7.68\n",
            "\n",
            "üè¢ Walt Disney Studios Stats:\n",
            "   Total Movies: 218\n",
            "   Total Gross: $38,546,931,771\n",
            "   Avg Opening Revenue: $69,577,768\n",
            "   Market Share: 19.64%\n",
            "\n",
            "üìä Predictions from all models:\n",
            "   ‚Ä¢ XGBoost: $125,593,552\n",
            "   ‚Ä¢ LightGBM: $129,777,721\n",
            "   ‚Ä¢ CatBoost: $105,935,603\n",
            "   ‚Ä¢ Random Forest: $132,071,878\n",
            "   ‚Ä¢ Gradient Boosting: $166,117,328\n",
            "   ‚Ä¢ Stacking Ensemble: $134,582,445\n",
            "\n",
            "   üéØ Ensemble Average: $132,346,421\n",
            "\n",
            "================================================================================\n",
            "üé¨ PREDICTION RESULTS FOR 'AVATAR: FIRE AND ASH'\n",
            "================================================================================\n",
            "\n",
            "üéØ Predicted Opening Revenue (Gradient Boosting): $166,117,328\n",
            "üéØ Ensemble Average Prediction: $132,346,421\n",
            "\n",
            "üìä Prediction Context:\n",
            "   ‚Ä¢ Release Date: December 19, 2025 (Friday, Holiday Season)\n",
            "   ‚Ä¢ Budget: $350,000,000\n",
            "   ‚Ä¢ Director: James Cameron (Avg Opening: $88,431,457)\n",
            "   ‚Ä¢ Distributor: Walt Disney Studios (Avg Opening: $69,577,768)\n",
            "\n",
            "üìà Avatar Franchise Comparison:\n",
            "   ‚Ä¢ Avatar (2009): Opening = $137,094,001\n",
            "   ‚Ä¢ Avatar (2010): Opening = $137,094,001\n",
            "   ‚Ä¢ Avatar (2010): Opening = $5,519,900\n",
            "   ‚Ä¢ Avatar: The Way of Water (2022): Opening = $197,681,686\n",
            "   ‚Ä¢ Avatar (2022): Opening = $13,896,763\n",
            "   ‚Ä¢ Avatar: The Way of Water (2023): Opening = $197,681,686\n",
            "\n",
            "üìä 95% Prediction Interval:\n",
            "   Lower Bound: $30,732,872\n",
            "   Upper Bound: $897,897,413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc337c89"
      },
      "source": [
        "### Step 9: Save Model and Results\n",
        "\n",
        "This cell handles the persistence of the trained models and the generated results, ensuring reproducibility and the ability to deploy or further analyze the project outputs.\n",
        "\n",
        "1.  **Best Model Saving:** The `best_model` (identified in Step 6) is saved to a file named `best_opening_revenue_model.pkl` using `joblib.dump`. This allows the model to be loaded and used later for inference without retraining.\n",
        "2.  **All Models Saving:** A dictionary containing all individual and ensemble models trained during the process is saved as `all_models_ensemble.pkl`. This is useful for future experimentation or for building custom ensemble predictions.\n",
        "3.  **Feature List Saving:** The list of `available_features` used to train the models is saved to `model_features.txt`. This is crucial for consistency, ensuring that any new data for prediction is preprocessed and ordered with the exact same features.\n",
        "4.  **Prediction Results Saving:** A summary of the 'Avatar: Fire and Ash' prediction, including the best model's prediction, ensemble average, prediction interval bounds, and the best model's performance metrics, is saved to `avatar_prediction_results.csv`.\n",
        "5.  **Model Comparison Saving:** The `comparison_df`, which details the performance of all models, is saved to `model_comparison_results.csv`.\n",
        "\n",
        "These saved files represent the tangible outcomes of the modeling phase and are ready for deployment, sharing, or further analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 9: SAVE MODEL AND RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 9: SAVING MODEL AND RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save the best model\n",
        "import joblib\n",
        "joblib.dump(best_model, 'best_opening_revenue_model.pkl')\n",
        "print(f\"‚úÖ Best model ({best_model_name}) saved as 'best_opening_revenue_model.pkl'\")\n",
        "\n",
        "# Save all models for ensemble predictions\n",
        "all_models = {name: res['model'] for name, res in results.items()}\n",
        "joblib.dump(all_models, 'all_models_ensemble.pkl')\n",
        "print(\"‚úÖ All models saved as 'all_models_ensemble.pkl'\")\n",
        "\n",
        "# Save feature list\n",
        "with open('model_features.txt', 'w') as f:\n",
        "    f.write('\\n'.join(available_features))\n",
        "print(\"‚úÖ Feature list saved as 'model_features.txt'\")\n",
        "\n",
        "# Save prediction results\n",
        "results_dict = {\n",
        "    'movie': 'Avatar: Fire and Ash',\n",
        "    'best_model': best_model_name,\n",
        "    'predicted_opening_revenue': avatar_prediction,\n",
        "    'ensemble_avg_prediction': ensemble_avg,\n",
        "    'prediction_lower_bound': max(0, prediction_low),\n",
        "    'prediction_upper_bound': prediction_high,\n",
        "    'model_r2_score': test_metrics['r2'],\n",
        "    'model_mae': test_metrics['mae'],\n",
        "    'model_rmse': test_metrics['rmse'],\n",
        "    'model_mape': test_metrics['mape']\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame([results_dict])\n",
        "results_df.to_csv('avatar_prediction_results.csv', index=False)\n",
        "print(\"‚úÖ Prediction results saved as 'avatar_prediction_results.csv'\")\n",
        "\n",
        "# Save model comparison\n",
        "comparison_df.to_csv('model_comparison_results.csv', index=False)\n",
        "print(\"‚úÖ Model comparison saved as 'model_comparison_results.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e61ny9prTU4",
        "outputId": "8c2c3534-5c9e-45de-e6f0-e99b30168d8c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 9: SAVING MODEL AND RESULTS\n",
            "================================================================================\n",
            "‚úÖ Best model (Gradient Boosting) saved as 'best_opening_revenue_model.pkl'\n",
            "‚úÖ All models saved as 'all_models_ensemble.pkl'\n",
            "‚úÖ Feature list saved as 'model_features.txt'\n",
            "‚úÖ Prediction results saved as 'avatar_prediction_results.csv'\n",
            "‚úÖ Model comparison saved as 'model_comparison_results.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f20c3c1"
      },
      "source": [
        "### Step 10: Create Visualizations\n",
        "\n",
        "This cell generates several insightful plots to visually analyze the model's performance and present the prediction results.\n",
        "\n",
        "1.  **Actual vs. Predicted Plot:** A scatter plot comparing the `y_test_orig` (actual values from the test set) against `y_pred_test` (predictions from the `best_model`). A red dashed line representing perfect prediction helps to visualize the model's accuracy and identify any systemic biases.\n",
        "2.  **Residual Distribution:** A histogram showing the distribution of the residuals (errors) on the original scale (`y_test_orig - y_pred_test`). A healthy model typically has residuals centered around zero, often following a normal distribution.\n",
        "3.  **Model Comparison Bar Chart:** A horizontal bar chart displaying the `Test R¬≤` scores for all models compared. The `best_model` is highlighted to emphasize its superior performance.\n",
        "4.  **Avatar Prediction Bar Chart:** A bar chart illustrating the predicted opening revenue for 'Avatar: Fire and Ash' from each individual model and the `ensemble_avg`. This helps visualize the consensus and variation across different models for this specific prediction.\n",
        "\n",
        "All plots are carefully designed for clarity and saved together in a single image file named `model_analysis.png`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# STEP 10: VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 10: CREATING VISUALIZATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 10.1 Actual vs Predicted Plot\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "# Plot 1: Actual vs Predicted\n",
        "ax1 = axes[0, 0]\n",
        "ax1.scatter(y_test_orig, y_pred_test, alpha=0.5, edgecolors='black', linewidth=0.5)\n",
        "ax1.plot([y_test_orig.min(), y_test_orig.max()], [y_test_orig.min(), y_test_orig.max()], 'r--', lw=2)\n",
        "ax1.set_xlabel('Actual Opening Revenue')\n",
        "ax1.set_ylabel('Predicted Opening Revenue')\n",
        "ax1.set_title(f'Actual vs Predicted - {best_model_name}')\n",
        "ax1.ticklabel_format(style='plain', axis='both')\n",
        "\n",
        "# Plot 2: Residual Distribution\n",
        "ax2 = axes[0, 1]\n",
        "residuals_orig = y_test_orig - y_pred_test\n",
        "ax2.hist(residuals_orig, bins=50, edgecolor='black', alpha=0.7)\n",
        "ax2.axvline(x=0, color='r', linestyle='--', lw=2)\n",
        "ax2.set_xlabel('Residuals (Actual - Predicted)')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.set_title('Distribution of Residuals')\n",
        "\n",
        "# Plot 3: Model Comparison Bar Chart\n",
        "ax3 = axes[1, 0]\n",
        "model_names = comparison_df['Model'].tolist()\n",
        "test_r2_scores = comparison_df['Test R¬≤'].tolist()\n",
        "colors = ['green' if name == best_model_name else 'steelblue' for name in model_names]\n",
        "bars = ax3.barh(range(len(model_names)), test_r2_scores, color=colors)\n",
        "ax3.set_yticks(range(len(model_names)))\n",
        "ax3.set_yticklabels(model_names)\n",
        "ax3.set_xlabel('Test R¬≤ Score')\n",
        "ax3.set_title('Model Comparison - Test R¬≤ Score')\n",
        "ax3.invert_yaxis()\n",
        "# Add value labels\n",
        "for i, (bar, score) in enumerate(zip(bars, test_r2_scores)):\n",
        "    ax3.text(score + 0.01, bar.get_y() + bar.get_height()/2, f'{score:.4f}', va='center')\n",
        "\n",
        "# Plot 4: All Model Predictions for Avatar\n",
        "ax4 = axes[1, 1]\n",
        "pred_names = list(all_predictions.keys()) + ['Ensemble Avg']\n",
        "pred_values = list(all_predictions.values()) + [ensemble_avg]\n",
        "colors = ['orange' if name == best_model_name or name == 'Ensemble Avg' else 'steelblue' for name in pred_names]\n",
        "bars = ax4.bar(range(len(pred_names)), pred_values, color=colors, edgecolor='black')\n",
        "ax4.set_xticks(range(len(pred_names)))\n",
        "ax4.set_xticklabels(pred_names, rotation=45, ha='right')\n",
        "ax4.set_ylabel('Predicted Opening Revenue ($)')\n",
        "ax4.set_title('Avatar: Fire and Ash - Predictions by Model')\n",
        "ax4.ticklabel_format(style='plain', axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, val in zip(bars, pred_values):\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2000000,\n",
        "             f'${val/1e6:.0f}M', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"‚úÖ Model analysis plot saved as 'model_analysis.png'\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T14:45:43.430384Z",
          "iopub.execute_input": "2025-12-15T14:45:43.430686Z",
          "iopub.status.idle": "2025-12-15T14:46:26.941185Z",
          "shell.execute_reply.started": "2025-12-15T14:45:43.430665Z",
          "shell.execute_reply": "2025-12-15T14:46:26.939852Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZZND04ckHWN",
        "outputId": "49a47e37-dfda-4e0c-d47b-0bad3b7a84c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 10: CREATING VISUALIZATIONS\n",
            "================================================================================\n",
            "‚úÖ Model analysis plot saved as 'model_analysis.png'\n"
          ]
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69d23c00"
      },
      "source": [
        "### Final Summary\n",
        "\n",
        "This cell provides a concise and comprehensive overview of the entire project's results, bringing together the key findings and the final prediction in an easily digestible format.\n",
        "\n",
        "1.  **Prediction Summary Box:** A well-formatted text box presents the most important results:\n",
        "    *   **Best Model:** Clearly states the name of the top-performing model.\n",
        "    *   **Predicted Opening Revenue:** Shows the `best_model`'s prediction for 'Avatar: Fire and Ash'.\n",
        "    *   **Ensemble Average:** Presents the average prediction across all models, often a more reliable estimate.\n",
        "    *   **Model Performance:** Summarizes the `best_model`'s performance metrics (R¬≤ Score, MAE, RMSE, MAPE) on the original scale of the target variable.\n",
        "    *   **95% Confidence Interval:** Provides the lower and upper bounds of the prediction interval for 'Avatar: Fire and Ash', indicating the range within which the true value is likely to fall.\n",
        "    *   **Franchise Context:** Lists the opening revenues of previous 'Avatar' movies for comparative analysis.\n",
        "\n",
        "2.  **Generated Files List:** A list of all output files created and saved throughout the notebook (`.pkl`, `.txt`, `.csv`, `.png`) is provided, making it easy to locate and access all project artifacts."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üé¨ FINAL SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë                    AVATAR: FIRE AND ASH - PREDICTION SUMMARY                  ‚ïë\n",
        "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
        "‚ïë                                                                              ‚ïë\n",
        "‚ïë  ÔøΩ BEST MODEL: {best_model_name:<30}\n",
        "‚ïë                                                                              ‚ïë\n",
        "‚ïë  üéØ PREDICTED OPENING REVENUE: ${avatar_prediction:>15,.0f}\n",
        "‚ïë  üéØ ENSEMBLE AVERAGE:          ${ensemble_avg:>15,.0f}\n",
        "‚ïë                                                                              ‚ïë\n",
        "‚ïë  üìä MODEL PERFORMANCE (with Log Transform):                                  ‚ïë\n",
        "‚ïë     ‚Ä¢ R¬≤ Score:  {test_metrics['r2']:.4f}\n",
        "‚ïë     ‚Ä¢ MAE:       ${test_metrics['mae']:>12,.0f}\n",
        "‚ïë     ‚Ä¢ RMSE:      ${test_metrics['rmse']:>12,.0f}\n",
        "‚ïë     ‚Ä¢ MAPE:      {test_metrics['mape']:.2f}%\n",
        "‚ïë                                                                              ‚ïë\n",
        "‚ïë  üìà 95% CONFIDENCE INTERVAL:                                                 ‚ïë\n",
        "‚ïë     ‚Ä¢ Lower: ${max(0, prediction_low):>15,.0f}\n",
        "‚ïë     ‚Ä¢ Upper: ${prediction_high:>15,.0f}\n",
        "‚ïë                                                                              ‚ïë\n",
        "‚ïë  üé¨ FRANCHISE CONTEXT:                                                       ‚ïë\n",
        "‚ïë     ‚Ä¢ Avatar (2009):           $137,094,001 opening                          ‚ïë\n",
        "‚ïë     ‚Ä¢ Avatar: Way of Water:    $197,681,686 opening                          ‚ïë\n",
        "‚ïë                                                                              ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n‚úÖ Script completed successfully!\")\n",
        "print(\"üìÅ Generated files:\")\n",
        "print(\"   ‚Ä¢ best_opening_revenue_model.pkl - Best trained model\")\n",
        "print(\"   ‚Ä¢ all_models_ensemble.pkl - All models for ensemble\")\n",
        "print(\"   ‚Ä¢ model_features.txt - Feature list\")\n",
        "print(\"   ‚Ä¢ avatar_prediction_results.csv - Prediction results\")\n",
        "print(\"   ‚Ä¢ model_comparison_results.csv - Model comparison\")\n",
        "print(\"   ‚Ä¢ feature_importance.png - Feature importance chart\")\n",
        "print(\"   ‚Ä¢ model_analysis.png - Model analysis visualizations\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpou2tVarb2c",
        "outputId": "63e079e0-428f-413f-b713-82a59e65ef04"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üé¨ FINAL SUMMARY\n",
            "================================================================================\n",
            "\n",
            "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
            "‚ïë                    AVATAR: FIRE AND ASH - PREDICTION SUMMARY                  ‚ïë\n",
            "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
            "‚ïë                                                                              ‚ïë\n",
            "‚ïë  ÔøΩ BEST MODEL: Gradient Boosting                                      \n",
            "‚ïë                                                                              ‚ïë\n",
            "‚ïë  üéØ PREDICTED OPENING REVENUE: $    166,117,328                   \n",
            "‚ïë  üéØ ENSEMBLE AVERAGE:          $    132,346,421                   \n",
            "‚ïë                                                                              ‚ïë\n",
            "‚ïë  üìä MODEL PERFORMANCE (with Log Transform):                                  ‚ïë\n",
            "‚ïë     ‚Ä¢ R¬≤ Score:  0.7286                                       \n",
            "‚ïë     ‚Ä¢ MAE:       $   8,350,308                                 \n",
            "‚ïë     ‚Ä¢ RMSE:      $  20,702,441                                \n",
            "‚ïë     ‚Ä¢ MAPE:      290.68%                                \n",
            "‚ïë                                                                              ‚ïë\n",
            "‚ïë  üìà 95% CONFIDENCE INTERVAL:                                                 ‚ïë\n",
            "‚ïë     ‚Ä¢ Lower: $     30,732,872                                  \n",
            "‚ïë     ‚Ä¢ Upper: $    897,897,413                                         \n",
            "‚ïë                                                                              ‚ïë\n",
            "‚ïë  üé¨ FRANCHISE CONTEXT:                                                       ‚ïë\n",
            "‚ïë     ‚Ä¢ Avatar (2009):           $137,094,001 opening                          ‚ïë\n",
            "‚ïë     ‚Ä¢ Avatar: Way of Water:    $197,681,686 opening                          ‚ïë\n",
            "‚ïë                                                                              ‚ïë\n",
            "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
            "\n",
            "\n",
            "‚úÖ Script completed successfully!\n",
            "üìÅ Generated files:\n",
            "   ‚Ä¢ best_opening_revenue_model.pkl - Best trained model\n",
            "   ‚Ä¢ all_models_ensemble.pkl - All models for ensemble\n",
            "   ‚Ä¢ model_features.txt - Feature list\n",
            "   ‚Ä¢ avatar_prediction_results.csv - Prediction results\n",
            "   ‚Ä¢ model_comparison_results.csv - Model comparison\n",
            "   ‚Ä¢ feature_importance.png - Feature importance chart\n",
            "   ‚Ä¢ model_analysis.png - Model analysis visualizations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d11ceff3"
      },
      "source": [
        "## Final Comments and Insights\n",
        "\n",
        "The project successfully built and evaluated several machine learning models to predict movie opening revenue, culminating in a robust prediction for 'Avatar: Fire and Ash'.\n",
        "\n",
        "**Key Insights and Results:**\n",
        "\n",
        "*   **Best Performing Model:** Gradient Boosting achieved the highest performance with a Test R¬≤ of 0.7286, a Test MAE of $8,350,308, a Test RMSE of $20,702,441, and a Test MAPE of 290.68%. This model showed good generalization capabilities, confirmed by 5-fold cross-validation (Mean CV R¬≤: 0.7912).\n",
        "*   **Prediction for 'Avatar: Fire and Ash':**\n",
        "    *   **Best Model Prediction:** $166,117,328\n",
        "    *   **Ensemble Average Prediction:** $132,346,421\n",
        "    *   **95% Prediction Interval:** Between $30,732,872 and $897,897,413.\n",
        "*   **Feature Importance:** Director's average opening revenue and distributor's average opening revenue were identified as the most influential features, highlighting the strong impact of past performance and industry relationships on a movie's box office success.\n",
        "*   **Data Transformation:** Log transformation of the target variable was crucial in handling the skewed nature of revenue data, significantly improving model performance.\n",
        "*   **Visualization:** Various plots (Actual vs. Predicted, Residual Distribution, Model Comparison, and Avatar Predictions) provide clear visual evidence of model performance and the specific prediction.\n",
        "\n",
        "All detailed results, including the best model, feature lists, and visualizations, have been saved to files:\n",
        "*   `best_opening_revenue_model.pkl`\n",
        "*   `all_models_ensemble.pkl`\n",
        "*   `model_features.txt`\n",
        "*   `avatar_prediction_results.csv`\n",
        "*   `model_comparison_results.csv`\n",
        "*   `feature_importance.png`\n",
        "*   `model_analysis.png`\n",
        "\n",
        "This project demonstrates a complete end-to-end machine learning pipeline for revenue prediction, from data preprocessing and feature engineering to multi-model evaluation and specific-case prediction with uncertainty estimation."
      ]
    }
  ]
}